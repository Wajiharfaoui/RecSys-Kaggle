{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendtion System Individual Project\n",
    "Prepared by: Wajih Arfaoui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\warfaoui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\warfaoui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from IESEGRecSys import eval\n",
    "from IESEGRecSys.model import ContentBased\n",
    "from surprise import KNNBasic, Reader, Dataset, SVD, CoClustering, BaselineOnly, accuracy\n",
    "from surprise.model_selection import GridSearchCV, cross_validate, KFold\n",
    "from sklearn.decomposition import PCA\n",
    "# NLP packages\n",
    "import nltk # pip install nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing all the libraries that we are going to need through the whole process, we start by importing our three datasets which are:  \n",
    "- **Meta** dataset: it contains additional item's data (title, description, image_url)  \n",
    "- **Train** dataset: it contains user-item ratings, including review text and additional user data  \n",
    "- **Test** dataset: it contains user-item ratings to predict  \n",
    "\n",
    "Once this done, I checked the cleanliness of our train data by:  \n",
    "- Checking the range values of the ratings that needs to be integers in the range [1,5].  \n",
    "- Checking the mission values especially for the columns: `userID`, `asin` and `overall`\n",
    "- Checking the data types per column.  \n",
    "\n",
    "And finally, we create a **Reader** object, with the attribute `rating_scale` which is a tuple with the lowest and highest possible range. It’s important to get this parameter right, otherwise parts of your data will be ignored. In our case, we have a minimum rating of 1.0 and a maximum rating of 5.0.  \n",
    "\n",
    "Next, I transform our train dataset into a **Surprise** format where it will become a sparse matrix, with the **users / items** are the **rows / columns**, and the **ratings** are elements in this matrix. \n",
    "Since I am going to use cross validation, I don't need to split my data, but instead, I will be using my whole dataset for training and cross-validate each time for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets \n",
    "meta = pd.read_csv(\"metadata.csv\")\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test_students.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pet Supplies                   2510\n",
       "Amazon Home                      29\n",
       "Tools & Home Improvement          7\n",
       "Health & Personal Care            5\n",
       "Grocery                           5\n",
       "Sports & Outdoors                 5\n",
       "Industrial &amp; Scientific       4\n",
       "Automotive                        3\n",
       "Cell Phones & Accessories         2\n",
       "Industrial & Scientific           2\n",
       "Toys & Games                      2\n",
       "Sports &amp; Outdoors             2\n",
       "Baby                              1\n",
       "Name: main_cat, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[\"main_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    107620\n",
       "4.0     23909\n",
       "3.0     14029\n",
       "1.0      8474\n",
       "2.0      7721\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check values for the ratings \n",
    "train[\"overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userID             0\n",
       "overall            0\n",
       "asin               0\n",
       "vote          145992\n",
       "reviewText         2\n",
       "summary            1\n",
       "style          29641\n",
       "image         157207\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userID          int64\n",
       "overall       float64\n",
       "asin           object\n",
       "vote           object\n",
       "reviewText     object\n",
       "summary        object\n",
       "style          object\n",
       "image          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check columns types \n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>userID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21069B00BFK2B24</td>\n",
       "      <td>21069</td>\n",
       "      <td>B00BFK2B24</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3506B00ZK0Y7R2</td>\n",
       "      <td>3506</td>\n",
       "      <td>B00ZK0Y7R2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21907B0002AQPA2</td>\n",
       "      <td>21907</td>\n",
       "      <td>B0002AQPA2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14092B0002DHXX2</td>\n",
       "      <td>14092</td>\n",
       "      <td>B0002DHXX2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3085B0006VB3SQ</td>\n",
       "      <td>3085</td>\n",
       "      <td>B0006VB3SQ</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  userID        asin  overall\n",
       "0  21069B00BFK2B24   21069  B00BFK2B24      0.0\n",
       "1   3506B00ZK0Y7R2    3506  B00ZK0Y7R2      0.0\n",
       "2  21907B0002AQPA2   21907  B0002AQPA2      0.0\n",
       "3  14092B0002DHXX2   14092  B0002DHXX2      0.0\n",
       "4   3085B0006VB3SQ    3085  B0006VB3SQ      0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"overall\"]=0.0\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1,5))\n",
    "data = Dataset.load_from_df(train[[\"userID\",\"asin\",\"overall\"]], reader)\n",
    "df_train = data.build_full_trainset()\n",
    "df_test = list(test[[\"userID\",\"asin\",\"overall\"]].itertuples(index = False , name = None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this project, I am going to apply **collaborative filtering** models to our dataset with the aim of finding similarities between **items / users** through commonly **rated** items.  \n",
    "Under this method, I will opt for these two main approaches:  \n",
    "- **Memory-based** models that calculate the similarities between **users / items** based on **user-item rating pairs** (I will use the `KNNBAsic`).  \n",
    "- **Model-based** models that use machine learning algorithms to estimate the ratings (I will use `SVD`, `ALS` and `CoClustering`).  \n",
    "\n",
    "In order to do the benchmarking of these models, and come out with the model with the best performance, I will opt for **Grid Search**.  \n",
    "\n",
    "`GridSearchCV` is an algorithm that we can import from `sklearn.model_selection` library, that automatically finds the best parameters for a particular model, what we call **hyperparameter tuning**.  \n",
    "\n",
    "To implement this algorithm, I started by creating a **dictionary** of all the parameters and their corresponding set of values that you want to test for best performance. \n",
    "Once the parameter dictionary is created, the next step is to create `GridSearchCV` for our model. I included **the model function** name as a estimator parameter. The **param_grid** as dictionary parameter, **the performance metrics** which I chose to be **RMSE** as a scoring parameter, and finally I precised the number of folds for cross validation for the cv parameter, which is 5 in this case.\n",
    "\n",
    "After fitting the model, I checked which were the parameters that return the highest accuracy. For the cases of where one of the parameters highest value was chosen in the best combination of parameters, I tried more values for that paremter, to see if performance further increases. \n",
    "After checking this, I printed the RMSE corresponding to the best combination of parameters that I am going to use later on to compare models to each others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User model hyperparameter tuning \n",
    "param_grid = {'k': [10,15,20,25,30],\n",
    "              'sim_options': {'name': [\"pearson\", 'cosine'],\n",
    "                              'user_based': [True]}\n",
    "              }\n",
    "knnbasic_gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], cv=5)\n",
    "knnbasic_gs.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': {'k': 30, 'sim_options': {'name': 'pearson', 'user_based': True}}}\n"
     ]
    }
   ],
   "source": [
    "# display the best parameter \n",
    "print(knnbasic_gs.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the best parameters rmse \n",
    "print(knnbasic_gs.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item model hyperparameter tuning \n",
    "param_grid = {'k': [5,10,15,20,25],\n",
    "              'sim_options': {'name': [\"pearson\", 'cosine'],\n",
    "                              'user_based': [False]}\n",
    "              }\n",
    "knnbasic_gs_i = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], cv=5)\n",
    "knnbasic_gs_i.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': {'k': 20, 'sim_options': {'name': 'pearson', 'user_based': False}}}\n"
     ]
    }
   ],
   "source": [
    "# display the best parameter \n",
    "print(knnbasic_gs_i.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.1824484369990143}\n"
     ]
    }
   ],
   "source": [
    "# display the best parameters rmse \n",
    "print(knnbasic_gs_i.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project, I am going to use another subgroup of collaborative filtering models which is **model-based**. Unlike **memory-based** models, These ones use machine learning algorithms.  \n",
    "In the upcoming steps, I am going to concentrate on the `SVD`, `ALS` and `CoClustering` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Singular value decomposition (SVD)** is a matrix factorisation technique, which reduces the number of features of a dataset by reducing the space dimension from N-dimension to K-dimension (where K< N). In our context, it aims to provide users with Amazon products’ recommendation from the latent features of item-user matrices. The code would show you how to use the SVD latent factor model for matrix factorization. \n",
    "The hyperparameters I considered for this method are:  \n",
    "\n",
    "1. **n_factors**: this parameter determines how many latent factors the model will try to find.  \n",
    "2. **n_epochs**: this parameter determines how many times the gradient descent calculations are repeated.  \n",
    "3. **lr_all**: it is the learning rate factor for all of the parameters. These are the step sizes the model will use to minimise the cost function.  \n",
    "4. **reg_all**: it is regularisation factor for all of the parameters.  \n",
    "5. **biased**: this parameter determines whether to choose biased or unbiased version of the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161753, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[\"userID\",\"asin\",\"overall\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD model hyperparameter tuning\n",
    "param_grid = {'n_factors':[300],'n_epochs': [200], 'lr_all':[0.02],'biased':[True],\n",
    "              'reg_all': [0.02]}\n",
    "svd_gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=300,refit=True)\n",
    "svd_gs.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': {'n_factors': 300, 'n_epochs': 200, 'lr_all': 0.02, 'biased': True, 'reg_all': 0.02}}\n"
     ]
    }
   ],
   "source": [
    "# display the best parameter \n",
    "print(svd_gs.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.0371928726286255}\n"
     ]
    }
   ],
   "source": [
    "# display the best parameters {'rmse': {'n_factors': 300, 'n_epochs': 200, 'lr_all': 0.02, 'biased': True, 'reg_all': 0.02}}\n",
    "print(svd_gs.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **The alternating least squares (ALS)** is a matrix factorization algorithm that uses Alternating Least Squares with Weighted-Lamda-Regularization (ALS-WR). It factors the user to item **matrix A** into the user-to-feature **matrix U** and the item-to-feature **matrix M**. It runs the ALS algorithm in a parallel fashion, and tries to find optimal factor weights to minimize the least squares between predicted and actual ratings.  \n",
    " Since the ALS uses baslines in the minimization objective function, I am going to use the `BaselineOnly` method from Surpise library, and I am going to configure it using these parameters:  \n",
    " 1. **reg_i**: The regularization parameter for products.  \n",
    " 2. **reg_u**: The regularization parameter for users.\n",
    " 3. **n_epochs**: The number of iteration of the ALS procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS model hyperparameter tuning\n",
    "param_grid = {'bsl_options': \n",
    "                {'reg_i':[5,10,15], 'reg_u':[5,10], 'n_epochs': [30,40]}}\n",
    "als_gs = GridSearchCV(BaselineOnly, param_grid, measures=['rmse'], cv=5)\n",
    "als_gs.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': {'bsl_options': {'reg_i': 10, 'reg_u': 5, 'n_epochs': 40}}}\n"
     ]
    }
   ],
   "source": [
    "# display the best parameter \n",
    "print(als_gs.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.0728335831007654}\n"
     ]
    }
   ],
   "source": [
    "# display the best parameters rmse \n",
    "print(als_gs.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Co-clustering** is a special case of clustering, where it is done simultaneously for the rows and columns of the matrix. It is basically a method of co-grouping users and items simultaneously based on similarity of their **pairwise interactions**.  In order to configure this model, I chose to tune the following parameters:  \n",
    "1. **n_cltr_u**: it represents the number of user clusters  \n",
    "2. **n_cltr_i**: it represents the number of products clusters  \n",
    "3. **n_epochs**: it determines the number of iteration of the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoClustering model hyperparameter tuning\n",
    "param_grid = {'n_cltr_u':[10,15], 'n_cltr_i':[10,15],'n_epochs': [10,20]}\n",
    "clust_gs = GridSearchCV(CoClustering, param_grid, measures=['rmse'], cv=5)\n",
    "clust_gs.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': {'n_cltr_u': 10, 'n_cltr_i': 10, 'n_epochs': 10}}\n"
     ]
    }
   ],
   "source": [
    "# display the best parameter \n",
    "print(clust_gs.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.2639709573502595}\n"
     ]
    }
   ],
   "source": [
    "# display the best parameters rmse \n",
    "print(clust_gs.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recommendation process is based on the similarity between those products. **Similarity** is measured based on the similarity in the content of those products and in our case, it will be based on the description column. \n",
    "First thing, I am going to do is to tokenize the `description` column to come up with a list of seperate words while just preserving alphabetic ones, then I am going to remove stopwords that don't have any importance when doing NLP, and we finish our preprocessing steps by applying the ``TfidfVectorizer`` in sci-kit learn to convert the collection of raw documents to a matrix of **TF-IDF** features. \n",
    "Before applying this function, I am going to put the hyperparameter `min_df` to 5 which means that ``TfidfVectorizer`` will ignore terms that have adocument frequency strictly lower than this threshold.\n",
    "\n",
    "Once our document-term-matrix is ready and tranformed into a Dataframe, we convert the ratings matrix into the surpirse dataframe format, and we move to fit our content based model.  \n",
    "The model I will be using is **ContentBased()** which is imported from `IESEGRecSys` library and has a single hyperparameter which is:  \n",
    "1. **NN**: by initiating this parameter, the model will filter the matrix for *k* nearest neighbors with non-negative similarity.  \n",
    "\n",
    "To fit the model, I will be using the **K-folds** cross validator, that will split my train dataset into *k* consecutive folds, each fold is then used once as a validation while the *k-1* remaining folds form the training set to fit the model on. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train[[\"userID\",\"asin\",\"overall\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta= meta.drop_duplicates(subset=['asin'])\n",
    "meta= meta[[\"asin\",'description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, case conversion & only alphabetic\n",
    "tokens = meta['description'].apply(lambda txt: [word.lower() for word in word_tokenize(str(txt)) if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup stop words list\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('nan')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# remove stopwords\n",
    "# stem\n",
    "token_stem = tokens.apply(lambda lst_token: [stemmer.stem(tok) for tok in lst_token if tok not in stop_words and len(tok) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2307, 2874)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF vectorizer\n",
    "tfidf = TfidfVectorizer(min_df=5)\n",
    "\n",
    "# apply tf-idf vectorizer -> document-term-matrix in sparse format\n",
    "dtm = tfidf.fit_transform([\" \".join(x) for x in token_stem])\n",
    "\n",
    "\n",
    "df_dtm = pd.DataFrame(dtm.toarray(), columns=tfidf.get_feature_names_out(), index=meta[\"asin\"])\n",
    "df_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train[\"asin\"].isin (df_dtm.index.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into a surprise dataframe\n",
    "reader = Reader(rating_scale = (1,5))\n",
    "dataset = Dataset.load_from_df(train, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the model\n",
    "cb = ContentBased(NN=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cross validation on the cb model\n",
    "\n",
    "kf = KFold(n_splits=100)\n",
    "rmse=[]\n",
    "for trainset, testset in kf.split(dataset):\n",
    "\n",
    "    # train and test algorithm.\n",
    "    cb.fit(df_dtm)\n",
    "    cb.fit_ratings(trainset)\n",
    "    predictions = cb.test(testset)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse.append(accuracy.rmse(predictions, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_rmse = pd.DataFrame(rmse,columns=[\"CB\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing hyperparameter tuning and fitting all our models, I calculated each model RMSE based on cross validation and as you can see above, we ended up by having the **SVD** model as the most appropriate model to use with a RMSE of 1.037 with these hyperparameter: **'n_factors': 300, 'n_epochs': 200, 'lr_all': 0.02, 'biased': True, 'reg_all': 0.02 and 300 cross validation folds** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IB_10</th>\n",
       "      <th>SVD_300</th>\n",
       "      <th>Clust_10</th>\n",
       "      <th>ALS_40</th>\n",
       "      <th>CB_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.182448</td>\n",
       "      <td>1.037193</td>\n",
       "      <td>1.263971</td>\n",
       "      <td>1.072834</td>\n",
       "      <td>1.129827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         IB_10   SVD_300  Clust_10    ALS_40     CB_50\n",
       "rmse  1.182448  1.037193  1.263971  1.072834  1.129827"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Benchmark different models\n",
    "models = {\"IB_10\":knnbasic_gs_i, \"SVD_300\":svd_gs,\"Clust_10\":clust_gs, \"ALS_40\":als_gs}\n",
    "bench = pd.concat([pd.DataFrame.from_dict(mod.best_score,orient='index') for mod in models.values()], axis=1,)\n",
    "bench.columns = list(models.keys())\n",
    "bench[\"CB_50\"] = cb_rmse[0]\n",
    "bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rating estimations\n",
    "test[\"overall\"] = pd.DataFrame(svd_gs.test(df_test))[\"est\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the test ratings estimation\n",
    "results = test[[\"ID\",\"overall\"]]\n",
    "results.to_csv('sample_sumbission_7.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81e0b57edd17c3833f38e5d4db6db81e2c0f982e51ee4890d9e870805c7d8a75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
